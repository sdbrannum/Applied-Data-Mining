---
title: 'Week 2: Decision Trees'
author: "San Cannon & Xuan Pham"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Packages Required  

The packages you will need for this week are **rpart**, **rpart.plot**, **e1071**, **class**, and **caret**.

# A Simple Example: Will a Customer Buy a Computer?

```{r cust_data}

ages <- c("Under30","Under30","30to40","Over40","Over40","Over40","30to40","Under30","Under30", "Over40", "Under30","30to40","30to40", "Over40")
income <-c("high","high","high","medium","low","low","low","medium","low", "medium", "medium", "medium","high","medium")
student <- c("no","no","no", "no","yes","yes","yes","no","yes","yes","yes", "no","yes", "no")
credit_rating <- c("fair","excellent","fair","fair","fair","excellent","excellent","fair","fair","fair","excellent","excellent","fair","excellent")
buys_computer<- c("no","no","yes","yes","yes","no","yes","no","yes","yes","yes","yes","yes", "no")
ages<-as.factor(ages)
income <-as.factor(income)
student<- as.factor(student)
credit_rating<-as.factor(credit_rating)
buys_computer<- as.factor(buys_computer)
cust_data <-data.frame(ages,income,student,credit_rating,buys_computer)
```


# Recursive Partitioning  

Decision trees are grown using a concept called **recursive partitioning**, or **top down, greedy, divide, and conquer approach**. Here is an overview of how recursive partitioning works:

1. Choose the attribute that is most predictive of the target variable.  
2. Observations in the training set are divided into groups of distinct values (i.e. categories). This form the first set of branches for the tree.  
3. Continue to divide and conquer the nodes, choosing the attribute with the most prediction power each time until one of three conditions occur: a) all observations for a given node belong to the same class; b) no more remaining attriutes for further partitioning; or c) no observations are left to classify.  

How do we determine the predictive power of an attribute at each splitting point? We can use a variety of heuristic or statistical measures. The most well-known decision tree algorithm is called **Classification and Regression Trees (CART)**, which is implemented in the **rpart** package. Hence, we will focus on the most popular statistical measures for selecting attributes in CART: **Information Gain (also called entropy measure)**, **Gain Ratio**, and **Gini Impurity Index**. 


# Statistical Measures of Impurity   

## Information Gain

**Entropy Measure**

$entropy(T) = - \sum_{k=1}^m p_k log2(p_k)$ 

**Information Gain**
$Gain(T,X) = Entropy(T) - Entropy(T, A)$

**Entropy** is a probabilistic measure of uncertainty or ignorance and **information** is a measure of the reduction in uncertainy. The entropy value can be as high as 1, when we have equal number of observations among the classes in the target variable. The lowest entropy value is 0, which means we have explained all the uncertainty in the training set.  


### Now Let's See How We Can Use Information Gain to Pick the First Attribute to Create Our Decision Tree (i.e. Most Important Predictor)  

In our training set, we have **9** customers who purchased a computer and **5** who did not. With no decision tree model, we calculate a total entropy of 0.94 for the target variable (purchase/not purchase a computer).

```{r total.entropy}
Ent_Buy<-((-9/14)*log2(9/14))-((5/14)*log2(5/14)) #entropy of the buy decision
print(Ent_Buy)
```

### What about Income as a Predictor?  

We can calculate the entropy value of each class in the Income predictor variable.  

```{r income.entropy}
Ent_Income_High<-((-2/4)*log2(2/4))-((2/4)*log2(2/4)) #four individuals with high income; two purchased; two did not.
print(Ent_Income_High)
Ent_Income_Medium<-((-4/6)*log2(4/6))-((2/6)*log2(2/6))
print(Ent_Income_Medium)
Ent_Income_Low<-((-3/4)*log2(3/4))-((1/4)*log2(1/4))
print(Ent_Income_Low)
```

The next step is to weigh each entropy value by the number of observations belonging to each class. The total entropy in the Income variable is 0.911. 

```{r}
Ent_IncomeBuy <- (4/14)*Ent_Income_High + (6/14)*Ent_Income_Medium + (4/14)*Ent_Income_Low
print(Ent_IncomeBuy)
```

If we use Income as a splitting variable, how much information have we gained with this predictor? A measly 0.029 (or 2.9%)!

```{r infogain.buy}

InfoGain_Income = Ent_Buy - Ent_IncomeBuy #information gained.
print(InfoGain_Income)
```

Should we look at the other predictors?


### What about Age? 

```{r entropy.age}
Ent_Age_Under30<-((-2/5)*log2(2/5))-((3/5)*log2(3/5))
print(Ent_Age_Under30)
Ent_Age_31_40<-((-4/4)*log2(4/4))
print(Ent_Age_31_40)
Ent_Age_Over40<-((-3/5)*log2(3/5))-((2/5)*log2(2/5))
print(Ent_Age_Over40)
Ent_AgeBuy <- (5/14)*Ent_Age_Under30 + (4/14)*Ent_Age_31_40 + (5/14)*Ent_Age_Over40
print(Ent_AgeBuy)

InfoGain_Age = Ent_Buy - Ent_AgeBuy #information gained.
print(InfoGain_Age)
```

The total information gained if we use Age as a predictor is 0.247 or 24.7%. Not bad! Can we do better?

### What about being a Student?

```{r entropy.student}

Ent_Student_Yes<-((-6/7)*log2(6/7))-((1/7)*log2(1/7))
print(Ent_Student_Yes)
Ent_Student_No<-((-3/7)*log2(3/7))-((4/7)*log2(4/7))
print(Ent_Student_No)
Ent_StudentBuy <- (7/14)*Ent_Student_Yes + (7/14)*Ent_Student_No
print(Ent_StudentBuy)

InfoGain_Student = Ent_Buy - Ent_StudentBuy #information gained.
print(InfoGain_Student)
```

The total information gained if we use Student status as a predictor is 0.152 or 15.2%. 

### What about Credit Rating Status?

```{r entropy.credit}
Ent_Credit_Excellent<-((-3/6)*log2(3/6))-((3/6)*log2(3/6))
print(Ent_Credit_Excellent)
Ent_Credit_Fair<-((-6/8)*log2(6/8))-((2/8)*log2(2/8))
print(Ent_Credit_Fair)
Ent_CreditBuy <- (6/14)*Ent_Credit_Excellent + (8/14)*Ent_Credit_Fair
print(Ent_CreditBuy)

InfoGain_Credit = Ent_Buy - Ent_CreditBuy #information gained.
print(InfoGain_Credit)
```

The total information gained if we use Credit Rating status is 0.048 or 4.8%. 

### Implementing Recursive Partitioning

Now that we have all the information gains calculated, recursive partitioning says that the most "predictive" predictor is used as the first split for the decision tree model. The predictors in the order of importance (and used to split the decision tree model) are Age, Student, Credit, and Income. 

```{r infogain_all}
print(InfoGain_Age)
print(InfoGain_Student)
print(InfoGain_Credit)
print(InfoGain_Income)
```

### Problems with the Information Gain Measure  

Information gain is biased towards choosing attributes with a large number of classes. 

## Gain Ratio  

The **Gain Ratio** can be used as an alternative to Information Gain. Gain ratio takes the size of the decision tree (i.e. number of branches) into account when choosing an attribute. The information gain is scaled using **split information**. If the split information is higher then the partitions are more or less the same size. If the split information is low then a few partitions have most of the observations.  


$SplitInfo(T,X) = - \sum_{k=1}^m \frac {p_k} k log2( \frac {p_k} k)$
$Gain Ratio = Gain(X) / SplitInfo(X)$

```{r splitinfo}
Split_Info_Income <- -((4/14)*log2(4/14))-((6/14)*log2(6/14))-((4/14)*log2(4/14)) #4 low income; 6 medium; 4 high
print(Split_Info_Income)
Gain_Ratio_Income <- (InfoGain_Income)/Split_Info_Income
```

How does the Gain Ratio for Income compared against the Information Gain? Notice that Income has three classes, and the Gain Ratio scales the information gain downward to mitigate bias. 

```{r compare}
Gain_Ratio_Income
InfoGain_Income
```

What about the other predictors?

```{r splitinfo.others}
Split_Info_Age <- -((5/14)*log2(5/14))-((4/14)*log2(4/14))-((5/14)*log2(5/14))
print(Split_Info_Age)
Gain_Ratio_Age <- (InfoGain_Age)/Split_Info_Age

Split_Info_Student <- -((7/14)*log2(7/14))-((7/14)*log2(7/14))
print(Split_Info_Student)
Gain_Ratio_Student <- (InfoGain_Student)/Split_Info_Student

Split_Info_Credit <- -((6/14)*log2(6/14))-((8/14)*log2(8/14))
print(Split_Info_Credit)
Gain_Ratio_Credit <- (InfoGain_Credit)/Split_Info_Credit
```

And now to compare Gain Ratio and Information Gain. Notice that Age is no longer the clear winner!

```{r}
Gain_Ratio_Age
InfoGain_Age #adjust downward because of three classes

Gain_Ratio_Student
InfoGain_Student #no difference

Gain_Ratio_Credit
InfoGain_Credit #no difference
```

### Problem with Gain Ratio  

Gain Ratio prefers unbalanced splits, where one group is much smaller than the other group(s).  

## Gini Index

**Gini index**

$Gini(T) = 1 - \sum_{k=1}^m p_k^2$

**Gini index** is a measure of impurity of the group of observations at each split. The highest Gini index depends on the number of classes in the target variable, and it can be calculated as $1- \frac 1 k$. The highest Gini index would occur if there are equal number of unclassified observations in all classes (k). The lowest Gini index is 0, which means all observations within each group belong to the same class. 

*Please note that the **rpart** package uses the Gini Index as the default statistical measure for choosing predictors!*  

### Overall Gini Index for Target Variable (buy a computer).  

```{r gini.buy}

Gini_Buy<-1-((9/14)^2+(5/14)^2)
print(Gini_Buy)
```

### Gini Index of Income

```{r gini.income}
Gini_Income_High<-1-((2/4)^2+(2/4)^2)
Gini_Income_Medium<-1-((4/6)^2+(2/6)^2)
Gini_Income_Low<-1-((3/4)^2+(1/4)^2)

print(Gini_Income_High)
print(Gini_Income_Medium)
print(Gini_Income_Low)
```

Now we have to weigh each Gini Index by the number of observations of each class in the target variable.  

```{r gini.income.overall}
Gini_Income <- (4/14)*Gini_Income_High + (6/14)*Gini_Income_Medium + (4/14)*Gini_Income_Low
print(Gini_Income)
```

How much reduction in impurity if we use Income as a predictor? About 0.019.   

```{r gini.income. reduction}
Gini_BuyIncome <- Gini_Buy - Gini_Income
print(Gini_BuyIncome)
```


### What About Age?  

A reduction in impurity of 0.116,

```{r gini.age}
Gini_Age_Under30<-1-((2/5)^2+(3/5)^2)
Gini_Age_31_40<-1-((4/4)^2+(0/4)^2)
Gini_Age_Over_40<-1-((3/5)^2+(2/5)^2)

print(Gini_Age_Under30)
print(Gini_Age_31_40)
print(Gini_Age_Over_40)

Gini_Age <- (5/14)*Gini_Age_Under30 + (4/14)*Gini_Age_31_40 + (5/14)*Gini_Age_Over_40
print(Gini_Age)

Gini_BuyAge <- Gini_Buy - Gini_Age
print(Gini_BuyAge)
```

### What about Being a Student?  

A reduction in impurity of 0.092.

```{r}
Gini_Student_Yes<-1-((6/7)^2+(1/7)^2)
Gini_Student_No<-1-((3/7)^2+(4/7)^2)

print(Gini_Student_Yes)
print(Gini_Student_No)

Gini_Student <- (7/14)*Gini_Student_Yes + (7/14)*Gini_Student_No
print(Gini_Student)

Gini_BuyStudent <- Gini_Buy - Gini_Student
print (Gini_BuyStudent)
```

### What about Credit Rating?  

A reduction in impurity of 0.429.

```{r}
Gini_Credit_Excellent<-1-((3/6)^2+(3/6)^2)
Gini_Credit_Fair<-1-((6/8)^2+(2/8)^2)

print(Gini_Credit_Excellent)
print(Gini_Credit_Fair)

Gini_Credit <- (6/14)*Gini_Credit_Excellent + (8/14)*Gini_Credit_Fair
print(Gini_Credit)

Gini_BuyCredit <- Gini_Buy - Gini_Credit
```

### Predictors   

Predictors in the order of predictive power: Age, Student status, Credit Rating, and Income. 

```{r gini.predictors}
print(Gini_BuyAge)
print(Gini_BuyStudent)
print(Gini_BuyCredit)
print(Gini_BuyIncome)
```

### Problem with Gini Index  

The Gini index is biased to multivalued attributes. The measure has difficulty when the number of classes is large. It also favors situations that result in equal-sized partitions and purity in both partitions.   

# CART Implementation  

So can we simplify all of the above calculations? Yes! We will use the Classification and Regression Tree (CART) implementation of decision tree models, which is in the **rpart** package.

```{r}
library(rpart)
library(rpart.plot)
set.seed(123)

# 80% train; 20% test
data_train <- cust_data[1:11,]
data_test  <- cust_data[12:14,]
prop.table(table(data_train$buys_computer))
prop.table(table(data_test$buys_computer))

data_rpart <- rpart(buys_computer ~ ., data=cust_data, method="class", 
                    parms=list(split="information"), 
                    control=rpart.control(minsplit = 1))

prp(data_rpart, type=1, extra=1, split.font=1, varlen = -10)
```

# Interpreting the Decision Tree Model  

Starting from the root node, we see that there are **9 customers who purchased a computer** and **5 customers who did not purchase a computer**. The dominant class is **yes purchased a computer**.  
  
First Split: *Is the customer Over40 or Under30?* If yes, go left. If no, go right. If you go right, notice that all 4 individuals purchased a computer. If you go left, you go to the next split.  

Second Split: *Is the customer a student?* If yes, go left. If no, go right. Notice that at this second split, we have **10 customers we have not classified yet. 5 purchased a computer and 5 did not purchased a computer.**  
  
Can you interpret the rest of this decision tree?  

## What about Using the Gini Index? 

```{r}
data_rpart <- rpart(buys_computer ~ ., data=cust_data, method="class", 
                    parms=list(split="gini"), 
                    control=rpart.control(minsplit = 1))

prp(data_rpart, type=1, extra=1, split.font=1, varlen = -10)
```

Unfortunately, there is no Gain Ratio implementation for rpart. 

# Let's Talk About Overfitting  

A problem with growing full tree models is that it is prone to **overfitting**. Overfitting occurs when we build a tree model that perfectly explains the training data but that's all it does! The model is so specific to the training data that it performs terribly when we use it to classify unknown cases (i.e. test data). The way to overcome overfitting is to **prune** the fully grown tree. 

## Cost complexity  

The cost complexity of a tree is composed of two parts: **Residual Sum of Squares** and **penalty factor for the size of the tree**. For a tree **T** that has **L(k)** terminal nodes, the cost complexity is written as:  

$Cost.Complexity(k) = RSS_k + \alpha L_k$  

Where $RSS_k$ = residual sum of squares of tree k (i.e. a measure of misclassification error)
      $\alpha$ = penalty factor for a tree size ranging from 0 to 1. An $\alpha$ of 0 indicates a fully grown tree. An $\alpha$ of 1 indicates a tree with only the root node (no splits).  
      
rpart calculates a similar measure to the Cost Complexity called the **complexity parameter (CP)**.  

$CP = \alpha/RSS_1$  

Where RSS_1 = residual sum of squares at the root node (i.e. before any split).  

The default stopping point for splitting a decision tree in rpart is CP = 0.01. If rpart reaches this CP value, it will stop splitting the decision tree. If rpart does not reach this CP value, things are a bit more complicated.  

      
## How rpart Uses CP to Pick the **Best Pruned Tree** if the CP > 0.01 

rpart uses an iterative process to find the **Best Pruned Tree**. Here's how it works:  

1. Partition the data into training and validation set. A training set includes observations that will be used to build the decision tree model. The validation set includes observations from the entire data set that will be used to "validate" the performance of a tree model.  
2. Grow the tree with the training data.  
3. Prune the tree successively, step by step, recording the complexity parameter (CP) at each tree size.     
4. Fit each pruned tree to the validation set and record the pruned tree that has the lowest misclassification rate.  
5. Repartition the data into new training and validation sets and redo Step #1 through #4 again. Calculate the average complexity parameters for all tree sizes.   
6. Repeat Steps #1 to #5 until an optimum CP value is reached.

```{r cpvalue}
printcp(data_rpart)
```

How is the optimum CP value reached? In the CP table above, we want to "zoom into" the cross-validation error **(xerror)** column. The tree size that has the the smallest cross validation error has 0 split and corresponds to a CP value of 0.30. 

To find this **Best Pruned Tree**, we need to look at the **estimated standard error of the cross validation error (xstd)**. When xerror = 1.0, we see that the xstd is 0.35857. We can find another tree that is **ONE standard error of the cross validation error** from the full tree. So we have **calculated acceptable xerror= 1.0 + 0.37417 = 1.37417**. A tree with a xerror value that is closest to the calculated acceptable xerror (without going over) is the first tree (nsplit = 0). Here is a visualization of the xerror + xstd. 

```{r cp.xstd}
cptable<-printcp(data_rpart)
cptable
plotcp(data_rpart, minline=TRUE, col="red") 
```

If we use the logic above, we see that the **Best Pruned Tree** is the first tree, which only include the root node! This is exactly what we get if we run rpart using all default settings. 

```{r}
data_rpart <- rpart(buys_computer ~ ., data=cust_data, method="class", 
                    parms=list(split="gini"))

prp(data_rpart, type=1, extra=1, split.font=1, varlen = -10)
```

You should not be alarmed when you see such an output! Take some time to print the cp value table. If you need to build a fuller tree, use the control() option in rpart. You should also get familiar with the rpart.control() help file.  

```{r}
#?rpart.control
```

# Returning to Our Opioid Prescriber Classification Problem 

## A reminder about the variables 

Here is the breakdown of the 331 variables:

Column 1: target variable; yes/no.  

Column 331: dummy variable for male (i.e. gender). 

Columns 241-291: state dummy variables. 

Columns 292-330: medical speciality dummy variables. 

Columns 2-240: number of prescriptions written for each non-opioid drug.


```{r}
# setwd("") # you can uncomment and fully specify the path where your files exist
prescribers<-read.csv("../Week1 - Naive Bayes, KNN/prescribers.csv") #or you can fully specify the pathname in the read command

#View(prescribers)

prescribers<-prescribers[,c(241,1:240,242:331)] #Rearranging the columns so that the target variable (opioid.prescriber) is in the first column

prescribers$Male <-ifelse(prescribers$Gender=="M",1,0) #if Male = 1; if Female=0.
prescribers<-prescribers[,-2] #We do not need the Gender variable anymore so delete it.

#names(prescribers)

dim(prescribers)

table(prescribers$Opioid.Prescriber)
```

```{r}

library(rpart)
library(rpart.plot)
set.seed(123)

prescribers_train<-prescribers[1:20000, ]
prescribers_test<-prescribers[20001:25000, ]

prop.table(table(prescribers_train$Opioid.Prescriber)) #notice the same proportion of opioid prescribers and non-prescribers.
prop.table(table(prescribers_test$Opioid.Prescriber))
```


# Using rpart to Build a Decision Tree

```{r}

set.seed(123)
prescribers_rpart <- rpart(prescribers_train$Opioid.Prescriber~., method="class", parms = list(split="gini"), data=prescribers_train)

prp(prescribers_rpart, type=1, extra=1, split.font=1, varlen = -10)
```

## Maybe something a bit fancier?

```{r}
rpart.plot(prescribers_rpart, type=0, extra=101)
```


The leaves show the opioid vs. non-opioid prescribers after the split at that node.   

The blue indicates the majority in that subgroup is not frequent opioid prescribers (negatives). The green indicates the majority in that subgroup is frequent opioid prescribers (positives).  

The label (yes or no) at each node indicates the label of the majority subgroup (for being a frequent opioid prescriber).  


## Vignette on using rpart.plot()

Check out this [vignette](http://www.milbo.org/rpart-plot/prp.pdf) on plotting rpart objects with rpart.plot(). A good resource for formatting your decision trees.


# Let's Look at the CP Table

rpart stops  at 10 splits where cp = 0.01, which also corresponds to the best pruned tree. Remember that cp = 0.01 is the default stopping point.

```{r}
cptable<-printcp(prescribers_rpart)
cptable
plotcp(prescribers_rpart, minline=TRUE, col="red") 
```


## Elbow Plot Method  

In the above cp plot, you can see there is an "elbow." There exists another rule of thumb that says to look at tree size around the elbow. The idea is that beyond the "elbow", we are decreasing the xerror but not by much. We can build a simpler tree without sacrificing too much xerror.  

How about a tree with 5 splits?

```{r elbowmethod}
set.seed(123)
prescribers_rpart_elbow <- rpart(prescribers_train$Opioid.Prescriber~., method="class", parms = list(split="gini"), control=rpart.control(maxdepth=5), data=prescribers_train)

rpart.plot(prescribers_rpart_elbow, type=0, extra=101)
```



