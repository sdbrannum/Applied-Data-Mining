---
title: 'HW #1'
author: "Steven Brannum"
date: '`r Sys.Date()`'
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data File Prep  

We will drop the ID and Zip.Code columns and also recode Education variable into a factor variable.
 
```{r data prep}
#bank.df <- read.csv("C:/Users/User/SkyDrive/SP18/Applied Data Mining - BIA 6301 BSB/HW1/HW1/HW1/UniversalBank.csv")
bank.df <- read.csv("D:/OneDrive/SP18/Applied Data Mining - BIA 6301 BSB/HW1/HW1/HW1/UniversalBank.csv")
bank.df <- bank.df[,-c(1,5)] # drop ID and zip code columns.  
# create categorical variable for education
bank.df$Education <- factor(bank.df$Education, levels = c(1,2,3), labels = c("Undergrad", "Graduate", "Advanced/Professional"))
```

## Logistic Regression Model
```{r logit}
logit.reg <- glm(bank.df$Personal.Loan ~., data = bank.df, family = "binomial")
options(scipen=999)
summary(logit.reg)
```

## New Customer
```{r new customer}
# put new customer info into data frame
newcus <- data.frame(Age=38, Experience=17, Income=150, Family=1, CCAvg=.2, Education="Graduate", Mortgage=0, Securities.Account=0, CD.Account=0, Online=1, CreditCard=1)
# predict probability against the model
predict(logit.reg, newcus, type="response")

```


```{r setup data and packages}
library(FNN)
library(e1071)
library(dummies)
library(class)
library(rpart)
library(rpart.plot)
#bank_p.df <- read.csv("C:/Users/User/SkyDrive/SP18/Applied Data Mining - BIA 6301 BSB/HW1/HW1/HW1/UniversalBankPlusNew.csv")
bank_p.df <- read.csv("D:/OneDrive/SP18/Applied Data Mining - BIA 6301 BSB/HW1/HW1/HW1/UniversalBankPlusNew.csv")
bank_p.df <- bank_p.df[, - c(1, 5)]
bank_p.df$Education <- factor(bank_p.df$Education, levels = c(1, 2, 3), labels = c("Undergrad", "Graduate", "Advanced/Professional"))
bank_p.df$Personal.Loan <- factor(bank_p.df$Personal.Loan, levels = c(0, 1), labels = c("NotInterested", "Interested"))


train_nb <- bank_p.df[1:4000,]
test_nb <- bank_p.df[4001:5000,]
newcus_nb <- bank_p.df[5001,]

```


## Naive Bayes
```{r bayes}


# training model
loan_nb <- naiveBayes(Personal.Loan ~ ., data = train_nb)

# testing model
nb_pred <- predict(loan_nb, test_nb)

head(test_nb)
actual <- test_nb$Personal.Loan


results.matrix.nb <- confusionMatrix(nb_pred, actual)

print(results.matrix.nb)


nb_tabs <- table(nb_pred, test_nb$Personal.Loan)
nb_tabs
prop.table(nb_tabs)

# predict against new customer
nb_pred_newcus <- predict(loan_nb, newcus_nb)
nb_pred_newcus


```


## KNN
```{r knn}
# turn labels back into numbers
# bank_p.df$Education <- factor(bank_p.df$Education, levels = c("Undergrad", "Graduate", "Advanced/Professional"), labels = c(1, 2, 3))

#rearrange and create new variable to hold data frame for knn
bank_knn.df <- bank_p.df
bank_knn.df <-bank_knn.df[,c(8,1:7,9:12)]


# turn education into dummy variables
bank_knn.df$Undergrad <- ifelse(bank_knn.df$Education == "Undergrad", 1, 0)
bank_knn.df$Grad <- ifelse(bank_knn.df$Education == "Graduate", 1, 0)
bank_knn.df$Advanced <- ifelse(bank_knn.df$Education == "Advanced/Professional", 1, 0)
# remove education col
bank_knn.df<- bank_knn.df[,-7]

# normalize data using min max
# age, experience, income, family, ccavg, mortgage
bank_nums <- bank_knn.df[, 2:7]
normalize <- function(x) { return((x - min(x)) / (max(x) - min(x))) }
bank_nums_n <- as.data.frame(lapply(bank_nums, normalize))

# test normalization
summary(bank_nums$Age)
summary(bank_nums_n$Age)

# combine normalized data with our other variables 
bank_knn_n <- cbind(bank_knn.df[, c(1, 8:14)], bank_nums_n[,])

# this removes incomplete rows, but since we have the customer to predict
# with the missing personal loan variable it'll throw it away
# bank_knn_n <- bank_knn_n[complete.cases(bank_knn_n),]

train_n_knn <- bank_knn_n[1:4000, 2:14]
test_n_knn <- bank_knn_n[4001:5000, 2:14]
newcus_n_knn <- bank_knn_n[5001, 2:14]


train_n_labels_knn <- bank_knn_n[1:4000, 1]
test_n_labels_knn <- bank_knn_n[4001:5000, 1]
newcus_n_labels_knn <- bank_knn_n[5001, 1]

# run model
set.seed(321)
loan_pred_knn <- knn(train = train_n_knn, test = test_n_knn, cl = train_n_labels_knn, k = 71)

knn_n_tabs <- table(test_n_labels_knn, loan_pred_knn)
knn_n_tabs

prop.table(knn_n_tabs)


# predict our new customer
knn_pred_newcus <- knn(train = train_n_knn, test = newcus_n_knn, cl = train_n_labels_knn, k = 71)
knn_pred_newcus # not interested


# trying with z score
head(bank_knn.df)
bank_knn_z <- as.data.frame(scale(bank_knn.df[-1]))

head(bank_knn_z)

train_n_knn_z <- bank_knn_z[1:4000,]
test_n_knn_z <- bank_knn_z[4001:5000,]
newcus_n_knn_z <- bank_knn_z[5001,]


train_n_labels_knn_z <- bank_knn_n[1:4000, 1]
test_n_labels_knn_z <- bank_knn_n[4001:5000, 1]
newcus_n_labels_knn_z <- bank_knn_n[5001, 1]

set.seed(321)
loan_pred_z <- knn(train = train_n_knn_z, test = test_n_knn_z, cl = train_n_labels_knn_z, k = 71)
knn_n_tabs_z <- table(test_n_labels_knn_z, loan_pred_z)
knn_n_tabs_z
prop.table(knn_n_tabs_z)


# predict our customer with z scores as normalization method
knn_pred_newcus_z <- knn(train = train_n_knn_z, test = newcus_n_knn_z, cl = train_n_labels_knn_z, k = 71)
knn_pred_newcus_z # not interested
```

## Decision Tree
```{r decision}
set.seed(321)
dt_train <- bank_p.df[1:4000,]
dt_test <- bank_p.df[4001:5000,]
dt_newcus <- bank_p.df[5001,]

# we want the training and test to be similiar in probability
prop.table(table(dt_train$Personal.Loan))
prop.table(table(dt_test$Personal.Loan))


# training model
data_rpart <- rpart(Personal.Loan ~ ., data = dt_train, method = "class", parms = list(split="information"), control = rpart.control(minsplit = 1))
prp(data_rpart, type = 1, extra = 1, split.font = 1, varlen = -10)
 pred_train <- predict(data_rpart, type="class")
prop.table(table(pred_train, dt_train$Personal.Loan))


# test data against training model
pred_test <- predict(data_rpart, dt_test, type = "class")
prop.table(table(pred_test, dt_test$Personal.Loan))


actual <- dt_test$Personal.Loan


results.matrix.nb <- confusionMatrix(pred_test, actual)

print(results.matrix.nb)


# customer against training model
pred_cus <- predict(data_rpart, dt_newcus, type = "class")
pred_cus

```


```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(ROCR)
library(knitr)
library(e1071)
library(class)
```

```{r rearrange and partionion }

    bankData <- bank_p.df[, c(8, 1:7, 9:12)]
    bankData <- bankData[1:5000,]

    trainIndex <- createDataPartition(bankData$Personal.Loan, p = .8, list = FALSE, times = 1)
    trainSet <- bankData[trainIndex,]
    testSet <- bankData[-trainIndex,]

    head(trainSet)

```

```{r nb compare }
options(warn=-1)

    cvCtrl <- trainControl(method = "cv", number = 3)
    set.seed(312)
bank_cv_nb <- train(Personal.Loan ~ ., data = trainSet, method = "naive_bayes", metric = "Accuracy", trControl = cvCtrl)
bank_cv_nb

actual <- testSet$Personal.Loan
predicted <- predict(bank_cv_nb, testSet, type = "raw")
table(predicted)
table(actual)
levels(actual)
levels(predicted)


results.matrix.dt <- confusionMatrix(predicted, actual, positive = "yes") # wtf
print(results.matrix.dt)
```

```{r knn compare }

set.seed(312)
cvCtrl <- trainControl(method = "cv", number = 3)
bank_cv_knn <- train(Personal.Loan ~ ., data = dt_train, method = "knn", metric = "Accuracy", trControl = cvCtrl)
bank_cv_knn

actual <- dt_test$Personal.Loan
predicted <- predict(bank_cv_knn, dt_test, type = "raw")
table(predicted)
table(actual)

results.matrix.knn <- confusionMatrix(predicted, actual, positive = "yes") # wtf
print(results.matrix.knn)
```


```{r dt compare}
head(bank_p.df)
trainIndex <- createDataPartition()
set.seed(312)
cvCtrl <- trainControl(method = "cv", number = 3)
bank_cv_dt <- train(Personal.Loan~., data=dt_train, method="rpart", metric="Accuracy", trControl=cvCtrl)
bank_cv_dt

actual <- dt_test$Personal.Loan
predicted <- predict(bank_cv_dt, dt_test, type = "raw")
table(predicted)
table(actual)

results.matrix.dt <- confusionMatrix(predicted, actual, positive = "yes") # wtf
print(results.matrix.dt)

```