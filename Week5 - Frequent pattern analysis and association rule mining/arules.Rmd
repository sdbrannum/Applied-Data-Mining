---
title: "Association Rules"
author: "Xuan Pham & San Cannon"
date: "`r Sys.Date()`"
output: html_document
---

# R Packages

The packages you will need to install for the week are **Matrix**, **arules** and **arulesViz**.

```{r packages}
library(Matrix)
library(arules)
library(arulesViz)
```


# Strawberry Pop-tarts During Hurricanes

If you read *Big Data* by Schonberger-Mayer and Cukier, then you are familiar with the Walmart story about stocking stores with strawberry pop-tarts during hurricanes. If not, here is the original [story](http://www.nytimes.com/2004/11/14/business/yourmoney/what-walmart-knows-about-customers-habits.html). We would expect that people buy sandbags, flashlights, and other similar items to prepare for hurricanes. Walmart figured out that people buy strawberry pop-tarts too!

Let's not forget the Target story about predicting pregnant customers. Target arrived at their answers by examining what known pregnant customers were buying throughout their pregnancies. The company then extrapolated the information onto other customers with unconfirmed pregnancy status. 

Our task for this evening is in the similar vein. We will be looking at datasets to see if we can find regularities in them.


# Association Rules Mining (Frequent Patterns Analysis)

We have examined several unsupervised learning models: k-means, k-modes, k-prototype, hierarchical clustering, and principal components analysis. We will examine another unsupervised learning method this week called **association rules** or **frequent pattern analysis (FPA)**.

Applications of FPA include analyses of market basket, DNA sequence, click stream analysis, and marketing activities (sale campaign; cross-marketing; etc.). We are going to examine two FPA algorithms: **Apriori** and **ECLAT**. **Apriori** is the more popular algorithm, but it can take up a lot of computational resources. **ECLAT** is a more efficient algorithm (i.e. faster) on smaller datasets.   

# Learning Goal for the Week

What interesting rules can we discover from 9,835 transactions containing 169 grocery products? In particular, if a customer purchases a particular grocery item (such as milk), what other grocery items would the customer likely to purchase as well? (i.e. bread, flowers, cheese, etc.)

# The Dataset

The *Groceries* dataset contains 9,835 transactions of 169 aggregated categories at a grocery store. The data was collected over a one month period. 

Source of dataset:

Michael  Hahsler,  Kurt  Hornik,  and  Thomas  Reutterer  (2006)  Implications  of  probabilistic  data modeling for mining association rules.  In M. Spiliopoulou, R. Kruse, C. Borgelt, A. Nuernberger, and W. Gaul, editors, From Data and Information Analysis to Knowledge Engineering, Studies in Classification, Data Analysis, and Knowledge Organization, pages 598-605. Springer-Verlag.


# Basic Concepts


Assume we have the following transaction database.

| ID | Items                             |
|----|-----------------------------------|
| 10 | Beer, Nuts, Diapers               |
| 20 | Beer, Coffee, Diapers             |
| 30 | Beer, Diapers, Eggs               |
| 40 | Nuts, Eggs, Milk                  |
| 50 | Nuts, Coffee, Diapers, Eggs, Milk |



An **itemset** is a list containing one or more items from the dataset. 

Here are all the possible 1-itemset from the database above:

1-itemset: {beer},{nuts},{diaper},{coffee},{eggs},{milk}

Here are all the possible 2-itemsets from the database above:

2-itemset: {beer, nuts}; {beer, diaper}; {beer, coffee}; {beer, eggs}; {beer, milk}; {nuts, diaper}; {nuts, coffee}; {nuts, eggs}; {nuts, milk}; {diaper, coffee}; {diapers, eggs}; {diapers, milk}; {coffee, eggs}; {coffee, milk}; {eggs, milk}

3-itemset and 4-itemset are created in similar fashions. 

**Absolute Support** is the count of occurrences of itemset X. For example, the absolute support for the 2-itemset {beer, diaper} is 3. The absolute support for the 2-itemset {eggs, milk} is 2.

**Relative Support** is the fraction of transactions that contain itemset X. For example, the relative support for the 2-itemset {beer, diaper} is 0.6

$Relative.Support = \frac{Count(X)}{n}=\frac{3}{5}=0.6$


If we try to generate all possible combinations of items, the computational time would grow exponentially. As a result, we impose a condition that we would only examine item combinations that occur with higher frequency in the databse. An itemset is said to be **frequent** if its support >= minimum support threshold (minsup). The minsup value is set by the user and should reflect business knowledge. For example, we may set a minimum support threshold of 0.05 (or 5%). We would only consider itemsets that occurred in 5% of the database.  


FPA results in a set of association rules. A typical association rule would state that given itemset X, then itemset Y is likely to occur. For example, {diaper} -> {beer}. Customers who purchase diapers are likely to purchase beer. Diaper is called the **antecedent**, and beer is called the **consequent** in this association rule. In other words, if we can compare an association rule to an IF-THEN statement, the IF is the antecedent and THEN is the consequent. 


Association rules are determined based on several quality measures:  

## 1. Support  

**Support**: How often does the rule happen in the database? In other words, the support of a rule is the **number of transactions that include both the antecedent and consequent itemsets**. Absolute support is the actual count of the rule as observed in the database. Relative support is the fraction (or percent) of the rule as observed in the database.    

## 2. Confidence  

**Confidence**: How often is the rule correct?  

$Confidence=\frac {support(X,Y)}{support(X)}$  

Confidence is the **ratio of the number of transactions with both antecedent and consequent itemsets and the number of transactions with antecedent itemset**.  


The user sets the **minimum support (minsup)** and **minimum confidence (minconf)** thresholds. **Rule interestingness** is determined by the minsup and minconf. Applied algorithms only report association rules that meet or exceed the minsup and minconf thresholds. 

For example, let's say we set the minsup = 50% and minconf=50%

Here are two association rules that are "interesting." 

```
{diaper}->{beer}

support(diaper,beer)=(count(diaper,beer))/N=3/5=0.6

confidence(diaper,beer)=(support(diaper,beer))/(support(diaper))=3/4=0.75

We report this rule as follows: diapers->beer (60%, 75%)
```


```
{beer}->{diaper} 

support(beer,diaper)=(count(beer,diaper))/N=3/5=0.6

confidence(beer,diaper)=support(beer,diaper)/support(3) =3/3=1.0

We report this rule as follows: beer -> diapers (60%, 100%)
```

Once we have identified the interesting rules, we want to be able to filter down to the strong rules. Strong rules are the ones that have high confidence. The problem with using "high confidence" as a benchmark is that we can get a high value for confidence even when the antecedent and consequent are independent. As long as there is a high level of support for the antecedent and consequent itemsets (separately), we would get a high confidence.  

## 3. Lift Ratio  


**Lift Ratio**: The lift ratio mesures the strength of a rule against random chance. The lift ratio overcomes the problem of getting a high confidence value even when the antecedent and consequent itemsets are independent. **We start by assuming that there is no association (i.e. complete independence) between the antecedent and consequent itemsets in a given rule.** We calculate the support for the antecedent itemsets and support for the consequent itemsets separately. We then use the calculated supports to derive a benchmark confidence.  

$Benchmark.Confidence=\frac{support(antecedent)*support(consequent)}{support(antecedent)}=support(consequent)$  

The benchmark confidence, thus, is nothing more than the support for the consequent itemsets. We can expressed the benchmark confidence in terms of relative support. 

$Benchmark.Confidencce=\frac{support(consequent)}{Total.Transactions.in.Database}$  

The lift ratio is then calculated as the ratio of confidence to the benchmark confidence.  

$Lift.Ratio=\frac{Confidence}{Benchmark.Confidence}$  

A lift ratio greater than 1 suggests that the level of association between the antecedent and consequent itemsets is higher than would be expected if they were independent from each other. The larger the lift ratio, the greater the strength of the association. 


For example:

```
Lift(honey --> whole milk) = Confidence (honey --> whole milk)/Support(milk)

Confidence(honey --> whole milk) = 0.7333

Support(whole milk) = 0.2556. 

Lift = 0.4108/0.2556 = 2.87
```


# How Do FPA Algorithms Work?

FPA algorithms utilize a search tree to generate frequent itemsets. A search tree starts with an empty itemset in its initialization. Using the minsup threshold established by the user, a set of candidate itemsets are generated. Support for each candidate itemset is then generated. If a search tree tries to generate all possible candidate itemsets, the process would be very computationally intensive for large datasets. As a result, most FPA algorithms utilize the **downward closure property**. Downward closure states that a supersede itemset cannot be frequent if its subset itemsets are not frequent. Consequently, most FPA algorithms will only generate candidates and count support for those itemsets that meet the downward closure property. 

Below is an illustration of the search tree that applies the downward closure property. We assume here that the minsup = 50%. Notice that no 3-itemset candidates are generated because only one 2-itemset {beer, diapers} is frequent. 

[search_tree_image](https://sites.google.com/site/xuanphamru/images/searchtree.jpg)


# Getting Started

The original data frame has 9,835 transactions with 169 grocery products. This translates into a sparse matrix with 9,835 rows and 169 columns.

```{r setup}
library(Matrix)
library(arules)

groceries <- read.transactions("groceries.csv", sep = ",") #9,835 transactions with 169 products.
```

Let's visualize the sparse matrix. You cannot make much sense of this picture.

```{r view_sparse}
image(groceries) 
```

Let's narrow it down to the first five transactions.

```{r view_sparse5}
image(groceries[1:5])
```

Another visualization. This time of a random sample of 100 transactions.

```{r sample_100}
image(sample(groceries, 100))
```

Can we quickly summarize the sparse matrix? Yes!

```{r summary}
summary(groceries)
```

Density indicates that only 2% of the elements are non-zero.

Mean is the average items per transaction.

Sum of "most frequent items" = total number of items bought in grocery dataset

Element is the frequency of a given number of items (i.e. 1, 2, 3, 4, etc.) bought in the transactions.

Here is another useful function that allows you to examine individual transactions.

```{r inspect}
inspect(groceries[1:5]) 
```

# Examining 1-itemset

Let's count the **support** (or frequency) of the grocery items. We will put the support in a data frame so we can view them.

```{r reshape}
freq_groceries_data_frame <- as.data.frame(itemFrequency(groceries))
head(freq_groceries_data_frame)
tail(freq_groceries_data_frame)
#View(freq_groceries_data_frame)
```

Let's pare this list down a bit to look at the first 15 items.

```{r counts}
itemFrequency(groceries[, 1:15])
```

Plotting the support.

```{r plotcounts}
itemFrequencyPlot(groceries) 
```

Too much information! Let's impose a rule. Minsup = 10%

```{r plotcounts.1}
itemFrequencyPlot(groceries, support = 0.1)
```

Here is a different take. Let's say we want to look at the "top 20" items.

```{r plotcounts_20}
itemFrequencyPlot(groceries, topN = 20) 
```


# Apriori Algorithm

The most frequently used FPA algorithm is Apriori. 

Pro: scalable for large datasets.

Con: computationally intensive. We have to keep comparing the candidate itemsets against the database until no frequent and/or candidate itemsets can be generated. 


**Data format requirement**: Horizontal. One column has the tid-set number (tid= transaction ID). Another column has a list of items.

**Method**

1. Initialize by scanning the database once to get frequent 1-itemset
2. Generate length (k+1) candidate itemsets from length k frequent itemsets
3. Test the candidate itemsets against the database. Prune candidate itemsets based on the minimum support threshold (minsup).
4. Terminate when no frequent or candidate set can be generated.

## Exploring apriori() in arules Package

```{r apriori_info}
#?apriori
```

### Default parameter settings

support = 0.1 (or 10%)
confidence = 0.8 (or 80%)
maxlen = maximum number of items in a rule. Default is 10.
minlen = minimum number of items in a rule. Default is 1.


Let's try the default parameter settings first.

```{r basic_rule}
apriori(groceries)
```

Not a single rule found! Let's try again with some tweakings to the parameter settings.

```{r grocery_rule} 
groceryrules <- apriori(groceries, parameter = list(support =
                          0.001, confidence = 0.8, minlen = 2)) 
```

Let's count the number of rules found

```{r see_rule}
print(groceryrules) 
```

## Evaluating Performance

Let's look at the number of rules and number of items per rule.

```{r summary_rules}
summary(groceryrules)
```

Let's see what we found:

3-itemset: 29 rules

4-itemset: 229 rules

5-itemset: 140 rules

6-itemset: 12 rules

Total = 410 rules. Whew!


Let's look at the first 10 rules. Please note the rules are not sorted in any order.

```{r inspect_grocery}
inspect(groceryrules[1:10]) 
```



## Improving Performance

Let's sort the rules by lift.

```{r sort_grocery}
groceryrules_sorted <- sort(groceryrules, by = "lift")
inspect(groceryrules_sorted[1:10])
```

And now by lift and support.

```{r sort_grocery_2}
groceryrules_sorted <-sort(groceryrules, by = c("lift", "support"))
inspect(groceryrules_sorted[1:10])
```

### Strong Rules. Actionable Rules.

A **strong** rule has high support and lift.

An **actionable** rule is one you can act on. 

Remember that there are always more trivial rules than non-trivial, actionable rules.

### An Example: Is it soup yet?

Here we are looking at the subsets of rules containing "soups" items. Winter just won't let go and we know people buy soup during colder months so let's assume they are still buying soup now.  What else are they buying with soup?

```{r soup}
soups_rules <- subset(groceryrules, items %in% "soups")
inspect(soups_rules)
```



# Another Example: Finding Partially Matched Rules

```{r pin}
hygiene_rules_partial <- subset(groceryrules, items %pin% "hygiene")
inspect(hygiene_rules_partial)
```

But we already know people buy milk!  What rules might we find that *don't* involve milk?
```{r milk_rules}
nomilk_rules <- subset(groceryrules, !(items %in% "whole milk"))
print(nomilk_rules)
inspect(nomilk_rules[1:10])


```
So now what can we learn about hygiene without it involving milk?

```{r nomilk_hygiene}
hygiene_rules_partial <- subset(nomilk_rules, items %pin% "hygiene")
print(hygiene_rules_partial)
inspect(hygiene_rules_partial)
```

# Removing Redundant Rules

From the arules manual:  

**A rule is redundant if a more general rule with the same or higher confidence exists. That is, a more specific rule is redundant if it is only equally or even less predictive than a more general rule. A general rule is more general if it has the same RHS [consequent] but one or more items removed from the LHS [antecedent]** (page 46).  


```{r prune}
#is.redundant(groceryrules) #gives a true/false logic for every rule

groceryrules_pruned<-groceryrules[!is.redundant(groceryrules)] #keeps only non-redundant rules

print(groceryrules_pruned)
```

# Targeting Items

```{r milk}
#What do people buy with whole milk?
rules_rhs_milk<-apriori(data=groceries, parameter=list(supp=0.001,conf = 0.08), 
               appearance = list(default="lhs",rhs="whole milk"))
rules_rhs_milk<-sort(rules_rhs_milk, decreasing=TRUE,by="lift")
inspect(rules_rhs_milk[1:5])
```

```{r milk_yogurt}
#What do people buy with "whole milk" and "yogurt"?
rules_lfs_milk_yogurt<-apriori(data=groceries, parameter=list(supp=0.001,conf = 0.08), 
               appearance = list(lhs=c("whole milk","yogurt"), default="rhs"))
rules_lfs_milk_yogurt<-sort(rules_lfs_milk_yogurt, decreasing=TRUE,by="lift")
inspect(rules_lfs_milk_yogurt[1:5])
```

Looking at the rules in a data frame.

```{r createdf}
rules_lfs_milk_yogurt_df <- as(rules_lfs_milk_yogurt, "data.frame")
#View(groceryrules_df)
head(rules_lfs_milk_yogurt_df)
```


# Using arulesViz Package to Visualize the "Mined" Rules

## Scatterplot

```{r groceryviz}
library(arulesViz)
plot(groceryrules)
```

##A two-key plot

Looking at the k-itemset rules by different coding colors.

order 3: 3 itemset rules
order 4: 4 itemset rules
and so on...

```{r groceryviz.2}
plot(groceryrules, shading="order", control=list(main="Two-key plot"))
```

## Grouped Matrix Plot

The rules are grouped using k-means clustering. Default quality measure is lift. Default plot shows 20 rules for the antecedents (LHS or left hand side).

```{r groceryviz.3}
plot(groceryrules, method="grouped")
```

Let's try 10 rules in LHS

```{r groceryviz.4}
plot(groceryrules, method="grouped", control=list(k=10)) 
```

## Graph Based Visualizations

This technique only works well for a small number of rules. We will create a graph for the first ten rules. Please note that we are using the sorted grocery rules vector. The default setting will give items and their relationships to each other.

```{r groceryrules.5}
plot(groceryrules_sorted[1:10], method="graph")
```

This is another graph type using the itemsets instead.

```{r groceryrules.6}
plot(groceryrules_sorted[1:10], method="graph", control=list(type="itemsets"))
```




# ECLAT Algorithm

Pro: Not as computationally intensive as Apriori

Con: Works best on smaller datasets

Required data format: Vertical


Support of a 1-itemset is the size of its tidset. Support of k-itemset is the intersection of the tidsets of the corresponding itemsets. For example, the support for {beer, diapers} is counted by matching up the tidsets of beer and diapers. 


| Beer | Nuts | Diapers | Coffee | Eggs | Milk |
|------|------|---------|--------|------|------|
| 10   | 10   | 10      | 20     | 30   | 40   |
| 20   | 40   | 20      | 50     | 40   | 50   |
| 30   | 50   | 30      |        | 50   |      |
|      |      | 50      |        |      |      |

We can see that as the size of the transaction database increases, it is more advantageous to count the tidsets than to pass through the databases multiple times like in Apriori.

Method

1. Generate 1-itemset candidate and count support at the same time
2. Prune candidates based on minsup threshold
3. Repeat Steps 1 & 2 until no more candidates can be generated or no frequent itemset is found


## Grocery Shopping in Belgium

We will use a dataset containing 88,162 grocery receipts from an anonymous Belgian supermarket. The receipts contained 16,470 SKUs. The data was collected between 1999 and 2000. More information about this dataset is available from [here](http://www.cs.rpi.edu/~zaki/Workshops/FIMI/data/retail.pdf).

Source of dataset:

Brijs T., Swinnen G., Vanhoof K., and Wets G. (1999), The use of association rules for product assortment decisions: a case study, in: Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining, San Diego (USA), August 15-18, pp.  254-260. ISBN: 1-58113-143-7.


We will download the dataset into R using a hyperlink. 

```{r read_eclat}
#retail <- read.transactions(file="http://fimi.ua.ac.be/data/retail.dat", sep = " ") 
retail <- read.transactions(file="retail.dat", sep = " ")#use this if you cannot access the hyperlink 
summary(retail)
```

Some questions for us to answer:

1. How many items does a typical receipt contain?  

2. Which SKUs appear most frequently?

3. What's the density of the sparse matrix? 


```{r view_eclat}
freq_retail <- as.data.frame(itemFrequency(retail))
head(freq_retail)
tail(freq_retail)
```

Sort the data frame above and find the support for the most popular SKUs. 

Default parameter settings for ECLAT: supp = 0.1 and maxlen = 5

What support and maxlen should we use? Many SKUs have support around 0.01. The mean number of items in receipt is 10.

```{r rules_eclat}
retail.rules<-eclat(retail, parameter=list(supp=0.01, maxlen=10))
print(retail.rules)
```

159 rules found! Let's dig in deeper. We can use inspect() to view all the rules. Let's just look at a few. 

```{r inspect_eclat}
inspect(retail.rules[1:10])
```

Alternatively, we can use ruleInduction() and set the confidence level to filter the rules. The default confidence=0.8

```{r sort_eclat}
retail.rules.review<-ruleInduction(retail.rules,retail)
retail.rules.sorted<-sort(retail.rules.review, by = c("lift", "confidence"))
inspect(retail.rules.sorted)
```

# Not just for shopping carts

### Describing cars 

  Car Evaluation Database was derived from a simple hierarchical
   decision model originally developed for the demonstration of DEX
   (M. Bohanec, V. Rajkovic: Expert system for decision
   making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates
   cars according to the following concept structure:

   CAR                      car acceptability
   . PRICE                  overall price
   . . buying               buying price
   . . maint                price of the maintenance
   . TECH                   technical characteristics
   . . COMFORT              comfort
   . . . doors              number of doors
   . . . persons            capacity in terms of persons to carry
   . . . lug_boot           the size of luggage boot
   . . safety               estimated safety of the car

```{r load_cars}
#if you want to use the ISLR library, use this code
library(ISLR)
data("Auto")
cars <- Auto

#if you want to use the file I posted, use this code
#cars <- read.csv("Auto.csv")


head(cars)
summary(cars)
```
### Reformat to categorical variables

```{r discretize}

cars$year <- factor(cars$year)
cars$origin <- factor(cars$origin, labels = c("US", "Europe", "Japan"))
cars$cylinders <- factor(cars$cylinders)
cars$gas <- factor(ifelse(cars$mpg <10, "Guzzler",
                   ifelse(cars$mpg <20, "Typical",
                           ifelse(cars$mpg < 30, "Better", 
                                   ifelse(cars$mpg <40 , "Great", "Awesome")))))
cars$go <- factor(ifelse(cars$acceleration <12, "Slug",
                   ifelse(cars$acceleration<16, "Average",
                           ifelse(cars$acceleration < 18, "Peppy", "Rocket"))))
cars$heavy <- factor(ifelse(cars$weight < mean(cars$weight), "No", "Yes"))
cars$displace_class <- with(cars, cut(displacement, breaks=quantile(displacement, probs=seq(0,1, by=0.25), na.rm=TRUE),  include.lowest=TRUE))
cars$hp_class <- with(cars, cut(horsepower, breaks=quantile(horsepower, probs=seq(0,1, by=0.2), na.rm=TRUE),  include.lowest=TRUE))
keep_carnames <- cars$name


#drop original variables
cat_cars <- cars[,-c(1,3,4,5,6,7,9)]


```


We can use these data dirctly (arules will take this input for the apriori algorithm) but there are lots of methods that won't be available to the data.frame

```{r transactionalize}
#itemFrequencyPlot(cat_cars)

#create transactions
tcars <- as(cat_cars, "transactions")
itemFrequencyPlot(tcars, topN = 20)
```

What can arules tell us?

```{r inspect_basic}
basic_rules <- apriori(tcars)
print(basic_rules)
inspect(basic_rules[1:10])
```
```{r sort_basic}
basic_rules_sorted <-sort(basic_rules, by = c("lift", "confidence"))
inspect(basic_rules_sorted[1:20])
```

So lots of technical stuff is related - are we surprised by this?  What if we want to know more about to look for to get great gas mileage?

```{r mileage_rules}
mileage_rules <- apriori(tcars, parameter = list(support =
                          0.1, confidence = 0.5, minlen = 2 ), appearance=list(rhs=c("gas=Great"))) 
inspect(sort(mileage_rules, by = c("lift", "confidence")))
```
Do we need all these rules?

```{r redundant_rules}

inspect(mileage_rules[!is.redundant(mileage_rules)])
```
I thought we pruned redundant rules - why is there a rule about cars from Japan and 4 cylindar cars from Japan?  What can we learn from that? 

What if we want to restrict the antecedent?

```{r japan_target}
japan_rules <- apriori(tcars, parameter = list(support =
                          0.1, confidence = 0.5, minlen = 2 ), appearance=list(lhs=c("origin=Japan"))) 
inspect(sort(japan_rules, by = c("lift", "confidence")))
```
Remember:  support is how many observations the rule can apply to and confidence is how many of those cases it does apply to.

```{r crosstabs}
#note the support for rule 4 is the proportion of Japanese cars that are not over the mean weight - which is the same as the proportion of Japanse cars so the confidence is 1
prop.table(table(cat_cars$origin))
prop.table(table(cat_cars$origin, cat_cars$heavy))

# the support for rule 1 is the proportion of Japanese cars with Great gas mileage.  The support for rule 1 is how often something with Great gas mileage is Japanese
prop.table(table(cat_cars$origin, cat_cars$gas))

# the lift for those two rules show that the relationships are stronger than what you would expect to see by chance. 

```


Other ways to look at rules with "Japan"

```{r japan_rules}
japan_rules <- subset(basic_rules_sorted, (items %pin% "Japan"))
print(japan_rules)
inspect(japan_rules)
```

```{r more_car_rules}
big_rules <- basic_rules <- apriori(tcars, parameter = list(confidence = 0.5, minlen = 2))
japan_rules_rhs <- subset(big_rules, (rhs %pin% "Japan"))
print(japan_rules_rhs)
inspect(japan_rules_rhs)
```

What is the difference between the two ways we singled out Japan?

More examples in the arules vignette: https://cran.r-project.org/web/packages/arules/vignettes/arules.pdf

And yet another example using the "Titanic" data
http://www.rdatamining.com/examples/association-rules


